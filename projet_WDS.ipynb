{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ADMDWc5LgpC0"
      },
      "source": [
        "# WSD par fine-tuning d'un modèle *BERT"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Import des librairies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xp3IN9YQexxx"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from tqdm.notebook import tqdm \n",
        "from random import shuffle\n",
        "import os\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "vfm0CXb9exyF"
      },
      "source": [
        "## Les données \"ASFALDA\"\n",
        "\n",
        "\n",
        "Il s'agit des données d'un FrameNet du français, comprenant environ 16000 annotations, pour environ 100 frames distincts.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "D_2cjFvYfOn3"
      },
      "source": [
        "### Chargement et Manipulation des données"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fYQhZaBzexyF"
      },
      "outputs": [],
      "source": [
        "## Fonction de chargement des données\n",
        "def load_asfalda_data(gold_data_file, split_info_file, val_proportion=None): \n",
        "    s = open(split_info_file)\n",
        "    lines = [ l[:-1].split('\\t') for l in s.readlines() ]\n",
        "    split_info_dic = { line[0]:line[1] for line in lines }\n",
        "\n",
        "    sentences = {'dev':[], 'train':[], 'test':[]}\n",
        "    tg_wrks = {'dev':[], 'train':[], 'test':[]}\n",
        "    tg_lemmas = {'dev':[], 'train':[], 'test':[]}\n",
        "    labels = {'dev':[], 'train':[], 'test':[]}\n",
        "\n",
        "    max_sent_len = {'dev':0, 'train':0, 'test':0}\n",
        "    max_tg_wrk = {'dev':0, 'train':0, 'test':0}\n",
        "\n",
        "    stream = open(gold_data_file)\n",
        "    for line in stream.readlines():\n",
        "        if line.startswith('#'):\n",
        "            continue\n",
        "        line = line.strip()\n",
        "        (sentid, tg_wrk, frame_name, tg_lemma, tg_pos, rest) = line.split('\\t',5)\n",
        "        sentence = rest.split(\"\\t\")[-1].split(' ')\n",
        "        part = split_info_dic[sentid]\n",
        "        tg_wrk = int(tg_wrk)\n",
        "\n",
        "        l = len(sentence)\n",
        "        sentences[part].append(sentence)\n",
        "        labels[part].append(frame_name)\n",
        "        tg_wrks[part].append(tg_wrk)\n",
        "        tg_lemmas[part].append(tg_lemma)\n",
        "        if max_sent_len[part] < l: \n",
        "            max_sent_len[part] = l \n",
        "        if max_tg_wrk[part] < tg_wrk: \n",
        "            max_tg_wrk[part] = tg_wrk \n",
        "    print(\"Longueur max des phrases:\", max_sent_len)\n",
        "    print(\"Rang max du target (en mots):\", max_tg_wrk)\n",
        "    \n",
        "    if val_proportion:\n",
        "        for dic in [sentences, tg_wrks, labels, tg_lemmas]:\n",
        "            (dic['val'], dic['train']) = split_list(dic['train'], proportion=val_proportion)\n",
        "    return sentences, tg_wrks, tg_lemmas, labels\n",
        "\n",
        "## Fonction de découpage des données\n",
        "def split_list(inlist, proportion=0.1, shuffle=False):\n",
        "     \"\"\" partitions the input list of items (of any kind) into 2 lists, \n",
        "     the first one representing @proportion of the whole \n",
        "     \n",
        "     If shuffle is not set, the partition takes one item every xxx items\n",
        "     otherwise, the split is random\"\"\"\n",
        "     n = len(inlist)\n",
        "     size1 = int(n * proportion)\n",
        "     if not(size1):\n",
        "          size1 = 1\n",
        "     print(\"SPLIT %d items into %d and %d\" % (n, n-size1, size1))\n",
        "     if shuffle:\n",
        "          inlist = sample(inlist, n)\n",
        "          return (inlist[:size1], inlist[size1:])\n",
        "     else:\n",
        "          divisor = int(n / size1)\n",
        "          l1 = []\n",
        "          l2 = []\n",
        "          for (i,x) in enumerate(inlist):\n",
        "               if i % divisor or len(l1) >= size1:\n",
        "                    l2.append(x)\n",
        "               else:\n",
        "                    l1.append(x)\n",
        "          return (l1,l2)\n",
        "\n",
        "## Chemin des données\n",
        "gold_data_file = './asfalda_data_for_wsd/sequoiaftb.asfalda_1_3.gold.uniq.nofullant.txt'\n",
        "split_info_file = './asfalda_data_for_wsd/sequoiaftb_split_info'\n",
        "\n",
        "## Chargement des données\n",
        "sentences, tg_wrks, tg_lemmas, label_strs = load_asfalda_data(gold_data_file,\n",
        "                                                              split_info_file, \n",
        "                                                              val_proportion=0.1)\n",
        "\n",
        "## Manipulation des données                                                    \n",
        "all_labels_strs = []\n",
        "all_lemma_strs = []\n",
        "for p in sentences.keys():\n",
        "    all_labels_strs += label_strs[p]\n",
        "    all_lemma_strs += tg_lemmas[p]\n",
        "    avgl = sum([len(s) for s in sentences[p]])/len(sentences[p])\n",
        "    print(\"%s : %d sentences, average lentgh=%.2f\" \n",
        "          %(p, len(sentences[p]), avgl))\n",
        "\n",
        "i2lemma = list(set(all_lemma_strs))\n",
        "lemma2i = {x:i for i,x in enumerate(i2lemma)}\n",
        "\n",
        "\n",
        "i2label = list(set(all_labels_strs))\n",
        "label2i = {x:i for i,x in enumerate(i2label)}\n",
        "i_OTHER_SENSE = label2i['Other_sense']\n",
        "\n",
        "labels = {}\n",
        "for p in label_strs.keys():\n",
        "    labels[p] = [label2i[x] for x in label_strs[p]]\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2Rgiftb-CQ5"
      },
      "source": [
        "### Baseline MFS (\"most frequent sense\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Calcul du dictionnaire des sens les plus communs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d2f7R9YD9_OH"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "## Fonction de calcul de la fréquence des sens\n",
        "def frequence(tg_lemmas,label_strs):\n",
        "  dict_lemmes = defaultdict(lambda: defaultdict(int))\n",
        "  dict_most_frequent = {}\n",
        "  for lemme, sens in zip(tg_lemmas,label_strs):\n",
        "      dict_lemmes[lemme][sens]+=1\n",
        "  for key, value in dict_lemmes.items():\n",
        "    max_item = max(value, key=lambda k: value[k])\n",
        "    dict_most_frequent[key] = max_item\n",
        "  return dict_lemmes, dict_most_frequent\n",
        "\n",
        "\n",
        "## Calcul de la fréquence des sens pour chaque ensemble\n",
        "dict_lemmes_train, dict_most_frequent_train = frequence(tg_lemmas['train']+tg_lemmas['val'],label_strs['train']+label_strs['val'])\n",
        "dict_lemmes_val, dict_most_frequent_val = frequence(tg_lemmas['val'],label_strs['val'])\n",
        "dict_lemmes_dev, dict_most_frequent_dev = frequence(tg_lemmas['dev'],label_strs['dev'])\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Evaluation du modèle MFS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lUM5bg-4rDI3"
      },
      "outputs": [],
      "source": [
        "## Fonction de calcul de l'accuracy du modèle de base\n",
        "def baseline(dict_most_frequent, tg_lemmas,label_strs,train_or_dev = 'train+val'):\n",
        "  amount_correct = 0\n",
        "  for lemme, sens in zip(tg_lemmas,label_strs):\n",
        "    if sens == dict_most_frequent[lemme]:\n",
        "      amount_correct +=1\n",
        "  return 'Accuracy of baseline model on ' + train_or_dev + ' is:',amount_correct/len(tg_lemmas)*100\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5qq0DQCJsmUd",
        "outputId": "7cb446fd-062d-4767-e9cb-8ef33f8a6852"
      },
      "outputs": [],
      "source": [
        "## Calcul de l'accuracy du modèle de base pour chaque ensemble\n",
        "baseline(dict_most_frequent_train, tg_lemmas['train']+tg_lemmas['val'],label_strs['train']+label_strs['val'])\n",
        "baseline(dict_most_frequent_dev, tg_lemmas['dev'],label_strs['dev'], 'dev')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0NvftzZs5TK"
      },
      "source": [
        "Now we need to find the lemmas, targets and the pairs of target + lemma that are not in train+val sets but are present in dev set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QyvuxcYjtZBd"
      },
      "outputs": [],
      "source": [
        "from itertools import chain\n",
        "\n",
        "lemmes_inconnus = []\n",
        "sens_inconnus = []\n",
        "\n",
        "for lemma in tg_lemmas['dev']:\n",
        "  if lemma not in chain(tg_lemmas['train'], tg_lemmas['val']):\n",
        "    lemmes_inconnus.append(lemma)\n",
        "for sens in label_strs['dev']:\n",
        "  if sens not in chain(label_strs['train'], label_strs['val']):\n",
        "    sens_inconnus.append(sens)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7m33grDnxGSw",
        "outputId": "6f3185ae-299c-4fce-f4c1-a4a7c29a984a"
      },
      "outputs": [],
      "source": [
        "'The number of unknown lemmas in dev is', len(set(lemmes_inconnus))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2z3h5B_ZxsLk",
        "outputId": "2f35758a-0095-4a68-8513-9df0588de55d"
      },
      "outputs": [],
      "source": [
        "'The number of unknown sens in dev is', len(set(sens_inconnus))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kLnlSS6tyZi2"
      },
      "outputs": [],
      "source": [
        "unknown_associations = []\n",
        "for lemma in dict_lemmes_dev.keys():\n",
        "  if lemma in dict_lemmes_train:\n",
        "    associations_possibles_train = dict_lemmes_train[lemma].keys()\n",
        "    associations_possibles_val = dict_lemmes_dev[lemma].keys()\n",
        "    for association_val in associations_possibles_val:\n",
        "      if association_val not in associations_possibles_train:\n",
        "         unknown_associations.append((lemma,association_val))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JbxiSKCp0X0z",
        "outputId": "032e8cb9-7013-45ea-c6b5-9c347268e153"
      },
      "outputs": [],
      "source": [
        "'The number of unknown associations in dev is', len(set(unknown_associations))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPbOlFK9nLoU"
      },
      "source": [
        "## Modèle et tokenization de type *BERT\n",
        "\n",
        "On va utiliser un modèle pré-entraîné de type *BERT, en passant par le module \"transformers\" d'huggingface."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G4DfehySexyR",
        "outputId": "75f28db4-c81c-4a21-cc61-26454db8915a"
      },
      "outputs": [],
      "source": [
        "## Installation des librairies nécessaires\n",
        "try:\n",
        "  import transformers\n",
        "except ImportError:\n",
        "  !pip install transformers\n",
        "\n",
        "try:\n",
        "    import sacremoses\n",
        "except ImportError:\n",
        "    !pip install sacremoses\n",
        "    \n",
        "\n",
        "from transformers import AutoModel, AutoTokenizer, AutoConfig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166,
          "referenced_widgets": [
            "a5611e1877c246e88f8c9ba7f73fcd14",
            "2a63baf0cdab419d985d39d07e0b376c",
            "b43699041dea4a7bb0f97ccf078e5c8b",
            "bc909bc1b05b4067a947a03dd2708a17",
            "6bebeb3da1ff494196ff8b3c5beb3cff",
            "a15458eba4fe4a3b93dcc3f9494990e3",
            "fd9fe2c7ea1c4d3b98de684243201383",
            "21b7360121934bc392a719c6eedef583",
            "cb2114aab6ab480cb6023fa0e5578c28",
            "ea2cbc25c2504383bbe9ecd164448d84",
            "67034dd7a099431d92976a1c2d237e31",
            "ff41d7c375274c64931a86c6d8c7a0bb",
            "70315c6a722e4176a2af4b631c755d36",
            "adb58c36931a412e802c295694410b9c",
            "1ffee7d44a3347adbc88c47208679bd6",
            "8fd286d4947644c5b7f1cc9cb8f5b179",
            "719b612ae5ab417d81876ae8744cf85f",
            "39f9c710ebd84b6fa81c4b9205c18a15",
            "23f0f80a2faa4228a1897b5b92562fff",
            "200efeb1a36f47b888af30721f1e8a97",
            "062aa7e54cce40c99ca2e1043f134b90",
            "ced61c53666f45f48fe301f2af4e5e35",
            "496703edea874c6fb8a22fd32e456081",
            "0a05f98a9e1a48729e833652dc413be5"
          ]
        },
        "id": "JJiKhfxhexyV",
        "outputId": "105f47fb-8666-4bb8-f59b-5e1538d231ae"
      },
      "outputs": [],
      "source": [
        "## Importation du modèle Flaubert\n",
        "flaubert_tokenizer = AutoTokenizer.from_pretrained(\"flaubert/flaubert_base_cased\")\n",
        "flaubert_config = AutoConfig.from_pretrained(\"flaubert/flaubert_base_cased\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMmtutNBexyQ"
      },
      "source": [
        "### Encodage des données (correspondance entre rang de mot et rang de token \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Pour pouvoir utiliser un modèle *BERT pré-entraîné, il faut\n",
        "- utiliser la même tokenisation en tokens (potentiellement des sous-mots) que celle utilisée à l'entraînement du modèle\n",
        "- convertir les séquences de tokens en séquences d'ids de tokens \n",
        "- et maintenir un lien entre les rangs de mot dans la phrase (dont le rang du target) et les rangs de tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GuLEbwS_tVtZ",
        "outputId": "5c89f1fb-2f48-4674-f6c6-e7be71cee1c1"
      },
      "outputs": [],
      "source": [
        "## Création de la classe WSDEncoder\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "class WSDEncoder:\n",
        "    def __init__(self, tokenizer, config):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.config = config \n",
        "    \n",
        "    ## Création des phrases et des positions des mots cibles\n",
        "    def new_ranks(self, sentences, tg_works):\n",
        "      tg_trks = []\n",
        "      phrases = []\n",
        "      for sent, rank in zip(sentences, tg_works):\n",
        "        has_seen = False\n",
        "       \n",
        "        word_gold = sent[rank]\n",
        "        phrase = []\n",
        "        for index, word in enumerate(sent):\n",
        "          if word == word_gold and has_seen == False:\n",
        "            tg_trks.append(len(phrase)+1)\n",
        "            has_seen = True\n",
        "          phrase.extend(flaubert_tokenizer.tokenize(word))\n",
        "        phrases.append(phrase)\n",
        "      \n",
        "      return phrases, tg_trks\n",
        " \n",
        "    ## Encodage des phrases\n",
        "    def encode(self, sentences, tg_wrks, max_length=350, verbose=False, is_split_into_words=True):\n",
        "      if is_split_into_words == False:\n",
        "        sentences = [sentence.split() for sentence in sentences]\n",
        "      phrases, first_trk_of_targets = self.new_ranks(sentences, tg_wrks)\n",
        "      tokenized = []\n",
        "      for phrase in phrases:\n",
        "         tokenized.append(flaubert_tokenizer.encode(phrase,add_special_tokens = True,truncation = True, max_length = max_length, padding = 'max_length', pad_to_max_length = True))\n",
        "      return tokenized, first_trk_of_targets\n",
        "\n",
        "encoder = WSDEncoder(flaubert_tokenizer, flaubert_config)\n",
        "\n",
        "test_sents = [\"Conséquemment leurs codes comprendraient des erreurs .\",\n",
        "            \"J' essaie de comprendre les transformers .\",  \n",
        "            \"Il n' a pas bien compris le code !\"]\n",
        "test_tg_wrks = [3, 3, 5] \n",
        "\n",
        "print(\"Not add padding and not add special tokens : \")\n",
        "tid_seqs, first_trk_of_targets = encoder.encode(test_sents, test_tg_wrks, max_length=100,is_split_into_words=False)\n",
        "for tid_seq, ft in zip(tid_seqs, first_trk_of_targets):\n",
        "    print(\"Len = %d target token rank = %d tid_seq = %s\" % (len(tid_seq), ft, str(tid_seq))) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ySTnpTLexyi"
      },
      "source": [
        "#### Test encodage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-qyWWzPl91PB",
        "outputId": "c2e6a87c-c419-43b1-b558-f72b6192af58"
      },
      "outputs": [],
      "source": [
        "encoder = WSDEncoder(flaubert_tokenizer, flaubert_config)\n",
        "test_sents = [[\"Conséquemment\", \"leurs\", \"codes\", \"comprendraient\", \"des\", \"erreurs\", \".\"]]\n",
        "test_tg_wrks = [3]\n",
        "tid_seqs, first_trk_of_targets = encoder.encode(test_sents, test_tg_wrks, max_length=20, verbose=True, is_split_into_words = True)\n",
        "\n",
        "print('trgs',first_trk_of_targets)\n",
        "\n",
        "for tid_seq, ft in zip(tid_seqs, first_trk_of_targets):\n",
        "  print(flaubert_tokenizer.convert_ids_to_tokens(tid_seq))\n",
        "  print(\"Len = %d target token rank = %d tid_seq = %s\" % (len(tid_seq), ft, str(tid_seq)))\n",
        "\n",
        "encoder.tokenizer.convert_ids_to_tokens(6090)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHApJ8mgexyn"
      },
      "source": [
        "### Classe WSDData: encodage complet des données asfalda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qMxpzaBhexyo"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "## Création de la classe WSDData\n",
        "class WSDData:\n",
        "    def __init__(self, corpus_type, sentences, tg_wrks, tg_lemmas, labels, encoder, max_length=350):\n",
        "        \n",
        "        self.corpus_type = corpus_type \n",
        "        self.size = len(sentences)\n",
        "        self.encoder = encoder\n",
        "\n",
        "        self.labels = labels       \n",
        "        self.sentences = sentences \n",
        "        self.tg_lemmas = tg_lemmas \n",
        "        \n",
        "        tid_seqs, first_trk_of_targets = encoder.encode(sentences, tg_wrks, max_length)\n",
        "        self.tg_lemma_indexes = [lemma2i[lemma]for lemma in self.tg_lemmas]\n",
        "\n",
        "        self.tid_seqs = tid_seqs  \n",
        "        self.first_trk_of_targets  = first_trk_of_targets\n",
        "        \n",
        "        \n",
        "    ## Mélange des données\n",
        "    def shuffle(self):\n",
        "      z = list(zip(self.labels, self.sentences, self.tg_lemma_indexes, self.tid_seqs, self.first_trk_of_targets))\n",
        "      random.shuffle(z)\n",
        "      labels, sentences, tg_lemma_indexes, tid_seqs,first_trk_of_targets = zip(*z)\n",
        "      \n",
        "      return labels, sentences, tg_lemma_indexes, tid_seqs,first_trk_of_targets\n",
        "\n",
        " \n",
        "    ## Création des batches sous forme de générateur\n",
        "    def make_batches(self, batch_size, shuffle_data=False):\n",
        "\n",
        "        bstart = 0\n",
        "        if shuffle_data:\n",
        "          self.labels, self.sentences, self.tg_lemmas, self.tid_seqs, self.first_trk_of_targets = self.shuffle()\n",
        "        N = len(self.labels)\n",
        "        while bstart < len(self.labels):\n",
        "          bend = min(bstart+batch_size,N)\n",
        "          b_labels, b_tid_seqs, b_tg_trks, b_lemmas = self.labels[bstart:bend] , self.tid_seqs[bstart:bend] , self.first_trk_of_targets[bstart:bend], self.tg_lemma_indexes[bstart:bend] \n",
        "          assert(len(b_labels)==len(b_tid_seqs))\n",
        "          yield (b_tid_seqs, b_tg_trks, b_labels, b_lemmas)\n",
        "        \n",
        "          bstart += batch_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m6D81AP4ye10",
        "outputId": "fcb341d3-576c-4c61-9a22-25914f804985"
      },
      "outputs": [],
      "source": [
        "## Création des données d'entraînement, de validation et de test\n",
        "MAX_LENGTH = 300\n",
        "wsd_data = {}\n",
        "for p in sentences.keys():\n",
        "    print(\"Encodage de la partie %s ...\" % p)\n",
        "    wsd_data[p] = WSDData(p, sentences[p], tg_wrks[p], tg_lemmas[p], labels[p], \n",
        "                          encoder, max_length=MAX_LENGTH)\n",
        "    for i, s in enumerate(wsd_data[p].tid_seqs):\n",
        "        if len(s) != MAX_LENGTH:\n",
        "            print(\"Size bug:\", i, s)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIw1BE1wexys"
      },
      "source": [
        "## Classe WSDClassifier : le réseau de neurones pour la WSD\n",
        "\n",
        "Architecture de base = \n",
        "- le modèle *BERT (ici FlauBERT)\n",
        "- puis une couche linéaire + softmax"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQ-w08okgaXv"
      },
      "source": [
        "### Le réseau : architecture, propagation avant, évaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KYAvfY83z0dF"
      },
      "outputs": [],
      "source": [
        "## Création de la classe MLP\n",
        "class MLP(nn.Module):\n",
        "  def __init__(self,input_size,output_size,hidden_size):\n",
        "    super(MLP, self).__init__()\n",
        "    self.encoder = nn.Linear(input_size,hidden_size)\n",
        "    self.decoder = nn.Linear(hidden_size,output_size)\n",
        "    self.activation = nn.Tanh()\n",
        "  def forward(self,xinput):\n",
        "    h = self.activation(self.encoder(xinput))\n",
        "    return self.decoder(h)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gFQEpfkRexyy"
      },
      "outputs": [],
      "source": [
        "## création de la classe WSDClassifier\n",
        "class WSDClassifier(nn.Module):\n",
        "    def __init__(self, num_labels, device='cpu', bert_model_name=\"flaubert/flaubert_base_cased\", freeze_bert = True, use_mlp = False, hidden_size = 100,nbr_lemmas = len(lemma2i), lemma_embedding_size = 518, add_lemmas = False):\n",
        "        super(WSDClassifier, self).__init__()\n",
        "        self.device = device\n",
        "        self.use_mlp = use_mlp\n",
        "        self.add_lemmas = add_lemmas\n",
        "        self.bert_layer = AutoModel.from_pretrained(bert_model_name,\n",
        "                                                   ).to(self.device)\n",
        "        self.bert_config = AutoConfig.from_pretrained(bert_model_name)\n",
        "        if self.add_lemmas:\n",
        "          self.hidden_size_bert = int(self.bert_config.hidden_size)+int(lemma_embedding_size)\n",
        "          print('lemmahidden', self.hidden_size_bert)\n",
        "        else:\n",
        "          self.hidden_size_bert = int(self.bert_config.hidden_size)\n",
        "        if add_lemmas:\n",
        "          self.lemma_embedding = nn.Embedding(nbr_lemmas, lemma_embedding_size).to(self.device)\n",
        "        if freeze_bert:\n",
        "          for param in self.bert_layer.parameters():\n",
        "            param.requires_grad = False\n",
        "        if self.use_mlp:\n",
        "          self.mlp = MLP(self.hidden_size_bert,num_labels,100).to(self.device)\n",
        "\n",
        "        else:\n",
        "          self.linear = torch.nn.Linear(self.hidden_size_bert,num_labels).to(self.device)\n",
        "        self.softmax = torch.nn.LogSoftmax(dim = 1).to(self.device)\n",
        "    \n",
        "    ## Fonction de prédictions\n",
        "    def forward(self, b_tid_seq, b_tg_trk, b_tg_lemma_indexes = None):\n",
        "        embeddings_bert = self.bert_layer(b_tid_seq, return_dict = True).last_hidden_state.to(self.device)\n",
        "        \n",
        "        embeddings_bert_tgt = embeddings_bert[torch.arange(embeddings_bert.size(0)), b_tg_trk].to(self.device) #last hidden state, ranks. We extract the \n",
        "        if self.add_lemmas: #add lemma information\n",
        "          lemma_embeddings = self.lemma_embedding(b_tg_lemma_indexes).to(self.device)\n",
        "          embeddings_bert_tgt = torch.cat((embeddings_bert_tgt,lemma_embeddings), dim = 1).to(self.device)#If we use lemmas, we concat bert embeddings and information about lemmas\n",
        "        \n",
        "        if self.use_mlp:\n",
        "          linear_tgt = self.mlp(embeddings_bert_tgt).to(self.device)\n",
        "        else:\n",
        "          linear_tgt = self.linear(embeddings_bert_tgt).to(self.device)\n",
        "        soft_tgt = self.softmax(linear_tgt).to(self.device)\n",
        "       \n",
        "        return soft_tgt\n",
        "    \n",
        "    ## Fonction d'entraînement\n",
        "    def run_on_dataset(self, wsd_data, optimizer, batch_size=32, validation_use = False):\n",
        "        pred_labels = []\n",
        "        val_losses = []\n",
        "        batch_acc = []\n",
        "        \n",
        "        loss_function = nn.NLLLoss()\n",
        "        self.eval()\n",
        "        for b_tid_seqs, b_tg_trks, b_labels, b_lemma_idx in wsd_data.make_batches(32, shuffle_data=False):\n",
        "          with torch.no_grad():\n",
        "            b_tid_seqs = torch.tensor(b_tid_seqs, device=self.device).to(self.device)\n",
        "            \n",
        "            b_tg_trks = torch.tensor(b_tg_trks, device=self.device).to(self.device)\n",
        "            b_labels = torch.tensor(b_labels, device=self.device).to(self.device)\n",
        "            b_lemma_idx = torch.tensor(b_lemma_idx, device=self.device).to(self.device)\n",
        "            log_probs = classifier(b_tid_seqs, b_tg_trks, b_lemma_idx).to(self.device)\n",
        "        \n",
        "            log_probs = self(b_tid_seqs, b_tg_trks,b_lemma_idx)\n",
        "            b_pred_labels = torch.argmax(log_probs, dim=1).to(self.device)\n",
        "            \n",
        "            pred_labels.extend(b_pred_labels)\n",
        "            \n",
        "            if validation_use:\n",
        "            \n",
        "              loss = loss_function(log_probs,b_labels)\n",
        "    \n",
        "              val_losses.append(loss.item())\n",
        "          \n",
        "          batch_acc.append(self.evaluate(b_labels,b_pred_labels))\n",
        "          \n",
        "\n",
        "\n",
        "        \n",
        "     \n",
        "        return pred_labels, val_losses, mean(batch_acc), b_labels, batch_acc\n",
        "\n",
        "    ## Fonction d'évaluation\n",
        "    def evaluate(self, gold_labels, pred_labels):\n",
        "        acc = float(torch.sum(gold_labels == pred_labels))/len(gold_labels)\n",
        "        return acc\n",
        "\n",
        "\n",
        "\n",
        "        \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vrkbQ4hVexy1",
        "outputId": "34d2ff8c-40b8-42e6-e28a-ffc867c22a0a"
      },
      "outputs": [],
      "source": [
        "num_labels = len(i2label)\n",
        "classifier = WSDClassifier(num_labels)\n",
        "\n",
        "for name, param in classifier.named_parameters():\n",
        "  print(\"PARAM named %s, of shape %s\" % (name, str(param.shape)))\n",
        "  print(param.requires_grad)\n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LeXSxEXexy5"
      },
      "source": [
        "#### Test de la propagation avant"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P4iIZvzDexy7",
        "outputId": "720d7ada-24a6-4ed7-f6a7-6e9ddd19d6bf"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    classifier.eval()\n",
        "    for b_tid_seqs, b_tg_trks, b_labels, _ in wsd_data['dev'].make_batches(32, shuffle_data=True):\n",
        "        b_tid_seqs = torch.tensor(b_tid_seqs, device=classifier.device)\n",
        "        b_tg_trks = torch.tensor(b_tg_trks, device=classifier.device)\n",
        "        b_labels = torch.tensor(b_labels, device=classifier.device).to(classifier.device)\n",
        "        print('input size : ',b_tid_seqs.size(),\" reference size : \",b_tg_trks.size())\n",
        "        log_probs = classifier(b_tid_seqs, b_tg_trks)\n",
        "        print('output size : ',log_probs.size(),\" reference size : \",b_labels.size())\n",
        "       \n",
        "        gold = b_labels[0] #.item()\n",
        "        print(\"GOLD LABEL of first ex %d ( = %s)\" % (gold, i2label[gold]))\n",
        "        print(\"LOG_PROBS before training: %s\\n\\n\" % str(log_probs[0]))\n",
        "        break\n",
        "        \n",
        "\n",
        "        \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aikfNi0zexy9"
      },
      "source": [
        "### Entraînement : fine-tuning sur la tâche de WSD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9p28nDcdYUqi",
        "outputId": "e681959b-5fef-45cb-ae26-471bb53ba15d"
      },
      "outputs": [],
      "source": [
        "from statistics import mean\n",
        "import numpy as np\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "LR = 0.0005\n",
        "n_epochs = 20\n",
        "import numpy as np\n",
        "stop_early = 0\n",
        "early_stopping_patience = 6\n",
        "\n",
        "\n",
        "loss_function = nn.NLLLoss() \n",
        "optimizer = optim.Adam(classifier.parameters(), lr=LR)\n",
        "\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    classifier.eval()\n",
        "    for b_tid_seqs, b_tg_trks, b_labels, _ in wsd_data['dev'].make_batches(32, shuffle_data=True):\n",
        "        b_tid_seqs = torch.tensor(b_tid_seqs, device=classifier.device)\n",
        "        b_tg_trks = torch.tensor(b_tg_trks, device=classifier.device)\n",
        "        b_labels = torch.tensor(b_labels, device=classifier.device).to(classifier.device)\n",
        "        print('input size : ',b_tid_seqs.size(),\" reference size : \",b_tg_trks.size())\n",
        "        log_probs = classifier(b_tid_seqs, b_tg_trks)\n",
        "        print('output size : ',log_probs.size(),\" reference size : \",b_labels.size())\n",
        "       \n",
        "        gold = b_labels[0] #.item()\n",
        "        print(\"GOLD LABEL of first ex %d ( = %s)\" % (gold, i2label[gold]))\n",
        "        print(\"LOG_PROBS before training: %s\\n\\n\" % str(log_probs[0]))\n",
        "        break\n",
        "        \n",
        "\n",
        "        \n",
        "\n",
        " = 'sequoiaftb.asfalda_1_3.wsd.lr' + 'Adam' + str(LR) + '_bs' + str(BATCH_SIZE)\n",
        "out_model_file = './' + config_name + '.model'\n",
        "out_log_file = './' + config_name + '.log'\n",
        "\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "val_accs = []\n",
        "min_val_loss = None\n",
        "epoch_losses = []\n",
        "train_data = wsd_data['train']\n",
        "val_data = wsd_data['val']\n",
        "dev_data = wsd_data['dev']\n",
        "classifier.train()\n",
        "early_stopping_patience = 0 \n",
        "stop_early = 6\n",
        "\n",
        "print('Training.....')\n",
        "acc = 0\n",
        "for epoch in range(n_epochs):\n",
        "  classifier.train()\n",
        "  \n",
        "  print('Training..... epoch nr: ', epoch)\n",
        "  for b_tid_seqs, b_tg_trks, b_labels, _ in train_data.make_batches(64, shuffle_data=True):\n",
        "    \n",
        "  \n",
        "    optimizer.zero_grad()\n",
        "    b_tid_seqs = torch.tensor(b_tid_seqs, device=classifier.device).to(classifier.device)\n",
        "    b_tg_trks = torch.tensor(b_tg_trks, device=classifier.device).to(classifier.device)\n",
        "    b_labels = torch.tensor(b_labels, device=classifier.device).to(classifier.device)\n",
        "    log_probs = classifier(b_tid_seqs, b_tg_trks).to(classifier.device)\n",
        "  \n",
        "    loss = loss_function(log_probs,  b_labels)\n",
        "    \n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    train_losses.append(loss.item())\n",
        "  epoch_losses.append(sum(train_losses)/len(train_losses))\n",
        "  \n",
        "  pred_labels, val_losses, val_acc, _, _ = classifier.run_on_dataset(val_data, optimizer, batch_size=32, validation_use=True)\n",
        "  print('--------')\n",
        "  print('train loss: ',epoch_losses[-1],'val accuracy: ', val_acc)\n",
        "  print('--------')\n",
        "  if val_acc>=acc:\n",
        "    acc=val_acc\n",
        "  else:\n",
        "    early_stopping_patience +=1\n",
        "    if early_stopping_patience == stop_early:\n",
        "      print('Stopping early...')\n",
        "      break\n",
        "\n",
        "print(\"train losses: %s\" % ' / '.join([ \"%.4f\" % x for x in epoch_losses]))\n",
        "print(\"val   losses: %s\" % ' / '.join([ \"%.4f\" % x for x in val_losses]))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "LIXDclLpntmM",
        "outputId": "7a67fa45-3617-4ee4-a3b1-24204c586160"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(epoch_losses)\n",
        "plt.ylabel(n_epochs)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "gbiznzHfntmM",
        "outputId": "6e37ceaf-8bef-44ba-f7de-576e8227cacc"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(val_accs)\n",
        "plt.ylabel(n_epochs)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZ3Hr8WplthG"
      },
      "source": [
        "We then added option lemmas, MLP and weights to balance weights.\n",
        "The results are seen below "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jjSKO4orlYbQ",
        "outputId": "a93cded2-9ec1-438d-95c9-3fa9da2d6772"
      },
      "outputs": [],
      "source": [
        "classifier_mlp_weights_lemmas = WSDClassifier(num_labels, use_mlp = True, hidden_size = 100,nbr_lemmas = len(lemma2i), lemma_embedding_size = 518, add_lemmas = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3nQUTyr5l4fj",
        "outputId": "e930755d-9dd1-4776-f3d7-3fdff97cdee6"
      },
      "outputs": [],
      "source": [
        "from statistics import mean\n",
        "import numpy as np\n",
        "stop_early = 0\n",
        "early_stopping_patience = 6\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "LR = 0.0005\n",
        "n_epochs = 30\n",
        "import numpy as np\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "class_weight = compute_class_weight(\"balanced\", np.unique(list(label2i.values())),list(label2i.values()))\n",
        "\n",
        "class_weight = torch.tensor(class_weight, dtype=torch.float).to(classifier.device)\n",
        "\n",
        "loss_function = nn.NLLLoss(weight = class_weight) \n",
        "optimizer = optim.Adam(classifier_mlp_weights_lemmas.parameters(), lr=LR)\n",
        "\n",
        "config_name = 'sequoiaftb.asfalda_1_3.wsd.lr' + 'Adam' + str(LR) + '_bs' + str(BATCH_SIZE)\n",
        "out_model_file = './' + config_name + '.model'\n",
        "out_log_file = './' + config_name + '.log'\n",
        "\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "val_accs = []\n",
        "min_val_loss = None\n",
        "epoch_losses = []\n",
        "train_data = wsd_data['train']\n",
        "val_data = wsd_data['val']\n",
        "dev_data = wsd_data['dev']\n",
        "\n",
        "val_accs = []\n",
        "\n",
        "print('Training.....')\n",
        "acc = 0\n",
        "for epoch in range(n_epochs):  \n",
        "  classifier_mlp_weights_lemmas.train()\n",
        "  \n",
        "  print('acc',acc)\n",
        "  print('Training..... epoch nr: ', epoch)\n",
        "  for b_tid_seqs, b_tg_trks, b_labels, b_lemma_idx in train_data.make_batches(64, shuffle_data=True):\n",
        "    \n",
        "  \n",
        "    optimizer.zero_grad()\n",
        "    b_tid_seqs = torch.tensor(b_tid_seqs, device=classifier_mlp_weights_lemmas.device).to(classifier_mlp_weights_lemmas.device)\n",
        "    b_tg_trks = torch.tensor(b_tg_trks, device=classifier_mlp_weights_lemmas.device).to(classifier_mlp_weights_lemmas.device)\n",
        "    b_labels = torch.tensor(b_labels, device=classifier_mlp_weights_lemmas.device).to(classifier_mlp_weights_lemmas.device)\n",
        "    b_lemma_idx = torch.tensor(b_lemma_idx, device=classifier_mlp_weights_lemmas.device).to(classifier_mlp_weights_lemmas.device)\n",
        "    log_probs = classifier_mlp_weights_lemmas(b_tid_seqs, b_tg_trks, b_lemma_idx).to(classifier_mlp_weights_lemmas.device)\n",
        "  \n",
        "    loss = loss_function(log_probs,  b_labels)\n",
        "    \n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    train_losses.append(loss.item())\n",
        "  epoch_losses.append(sum(train_losses)/len(train_losses))\n",
        " \n",
        "  pred_labels, val_losses, val_acc, _, _ = classifier_mlp_weights_lemmas.run_on_dataset(val_data, optimizer, batch_size=32, validation_use=True)\n",
        "  val_accs.append(val_acc)\n",
        "  print('--------')\n",
        "  print('train loss: ',epoch_losses[-1],'val accuracy: ', val_acc)\n",
        "  print('--------')\n",
        " \n",
        "  if val_acc>=acc:\n",
        "    acc=val_acc\n",
        "  else:\n",
        "    early_stopping_patience +=1\n",
        "    if early_stopping_patience == stop_early:\n",
        "      print('Stopping early...')\n",
        "      break\n",
        "\n",
        "print(\"train losses: %s\" % ' / '.join([ \"%.4f\" % x for x in epoch_losses]))\n",
        "print(\"val   losses: %s\" % ' / '.join([ \"%.4f\" % x for x in val_losses]))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "L7KhwZHXvnQc",
        "outputId": "b420b714-840c-431a-9e3e-13dd68779b0b"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(epoch_losses)\n",
        "plt.ylabel(n_epochs)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "O8v2EgUGvo_m",
        "outputId": "d141ae2e-807d-4113-c039-27bf142127b5"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(val_accs)\n",
        "plt.ylabel(n_epochs)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rsgc93t3XPkN"
      },
      "source": [
        "Below is a variant of classifier using class weight and MLP layer but NOT lemmas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GugrUBWavbnB"
      },
      "outputs": [],
      "source": [
        "classifier_mlp_weights = WSDClassifier(num_labels, use_mlp = True, hidden_size = 100,nbr_lemmas = len(lemma2i), lemma_embedding_size = 518, add_lemmas = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Tqpu49MvgrG",
        "outputId": "ecdc6a74-5dfe-4e05-d446-ce07d6546b26"
      },
      "outputs": [],
      "source": [
        "from statistics import mean\n",
        "import numpy as np\n",
        "stop_early = 0\n",
        "early_stopping_patience = 6\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "LR = 0.0005\n",
        "n_epochs = 25\n",
        "import numpy as np\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "class_weight = compute_class_weight(\"balanced\", np.unique(list(label2i.values())),list(label2i.values()))\n",
        "\n",
        "class_weight = torch.tensor(class_weight, dtype=torch.float).to(classifier.device)\n",
        "\n",
        "loss_function = nn.NLLLoss(weight = class_weight) \n",
        "optimizer = optim.Adam(classifier_mlp_weights.parameters(), lr=LR)\n",
        "\n",
        "config_name = 'sequoiaftb.asfalda_1_3.wsd.lr' + 'Adam' + str(LR) + '_bs' + str(BATCH_SIZE)\n",
        "out_model_file = './' + config_name + '.model'\n",
        "out_log_file = './' + config_name + '.log'\n",
        "\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "val_accs = []\n",
        "min_val_loss = None\n",
        "epoch_losses = []\n",
        "train_data = wsd_data['train']\n",
        "val_data = wsd_data['val']\n",
        "dev_data = wsd_data['dev']\n",
        "classifier_mlp_weights.train()\n",
        "\n",
        "print('Training.....')\n",
        "acc = 0\n",
        "for epoch in range(n_epochs):\n",
        "  classifier_mlp_weights.train()\n",
        "  \n",
        "  print('acc',acc)\n",
        "  print('Training..... epoch nr: ', epoch)\n",
        "  for b_tid_seqs, b_tg_trks, b_labels, b_lemma_idx in train_data.make_batches(64, shuffle_data=True):\n",
        "    \n",
        "  \n",
        "    optimizer.zero_grad()\n",
        "    b_tid_seqs = torch.tensor(b_tid_seqs, device=classifier_mlp_weights.device).to(classifier_mlp_weights.device)\n",
        "    b_tg_trks = torch.tensor(b_tg_trks, device=classifier_mlp_weights.device).to(classifier_mlp_weights.device)\n",
        "    b_labels = torch.tensor(b_labels, device=classifier_mlp_weights.device).to(classifier_mlp_weights.device)\n",
        "    b_lemma_idx = torch.tensor(b_lemma_idx, device=classifier_mlp_weights.device).to(classifier_mlp_weights.device)\n",
        "    log_probs = classifier_mlp_weights(b_tid_seqs, b_tg_trks, b_lemma_idx).to(classifier_mlp_weights.device)\n",
        "  \n",
        "    loss = loss_function(log_probs,  b_labels)\n",
        "    \n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    train_losses.append(loss.item())\n",
        "  epoch_losses.append(sum(train_losses)/len(train_losses))\n",
        "  \n",
        "  pred_labels, val_losses, val_acc, _, _ =classifier_mlp_weights.run_on_dataset(val_data, optimizer, batch_size=32, validation_use=True)\n",
        "  val_accs.append(val_acc)\n",
        "  print('--------')\n",
        "  print('train loss: ',epoch_losses[-1],'val accuracy: ', val_acc)\n",
        "  print('--------')\n",
        " \n",
        "  if val_acc>=acc:\n",
        "    acc=val_acc\n",
        "  else:\n",
        "    early_stopping_patience +=1\n",
        "    if early_stopping_patience == stop_early:\n",
        "      print('Stopping early...')\n",
        "      break\n",
        "\n",
        "print(\"train losses: %s\" % ' / '.join([ \"%.4f\" % x for x in epoch_losses]))\n",
        "print(\"val   losses: %s\" % ' / '.join([ \"%.4f\" % x for x in val_losses]))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "iICXMb1IwDbZ",
        "outputId": "8b1373c2-0905-4dfe-c329-0f6238b0ecda"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(epoch_losses)\n",
        "plt.ylabel(n_epochs)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "F93P9OlMwDbk",
        "outputId": "f5e8444c-c0f9-4c1c-b63c-70d974557239"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(val_accs)\n",
        "plt.ylabel(n_epochs)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHt6rTougG0K"
      },
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lYvpk9VHb4CD"
      },
      "outputs": [],
      "source": [
        "test_data = wsd_data['test']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XjSdDIGTf3a"
      },
      "source": [
        "**Normal version of classifier**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hHYxcJBNexzB",
        "outputId": "f88194d0-d169-4a75-ac61-b70f6246aafd"
      },
      "outputs": [],
      "source": [
        "pred_labels, val_losses, dev_acc, b_labels, _ = classifier.run_on_dataset(dev_data, optimizer, batch_size=32, validation_use=False)\n",
        "pred_labels, val_losses, test_acc, b_labels, _ = classifier.run_on_dataset(test_data, optimizer, batch_size=32, validation_use=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2VHBqVMXT0WW",
        "outputId": "1e0c7e43-1c79-4ebb-fbe4-501abb5dee69"
      },
      "outputs": [],
      "source": [
        "print('dev acc: ', dev_acc, 'test acc:', test_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZH92iHUaTjGA"
      },
      "source": [
        "**Classifier with mlp, lemmas and weights**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "unUsvJIJb_M5"
      },
      "outputs": [],
      "source": [
        "pred_labels, val_losses, dev_acc1, b_labels, _ = classifier_mlp_weights_lemmas.run_on_dataset(dev_data, optimizer, batch_size=32, validation_use=False)\n",
        "pred_labels, val_losses, test_acc1, b_labels, _ = classifier_mlp_weights_lemmas.run_on_dataset(test_data, optimizer, batch_size=32, validation_use=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y6RvXGnuT9qE",
        "outputId": "1651c3d4-2b10-47b8-b221-0fcceac76a00"
      },
      "outputs": [],
      "source": [
        "print('dev acc: ', dev_acc1, 'test acc:', test_acc1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58FEY1qRTnlh"
      },
      "source": [
        "**Classifier with weights**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ex411WDicACm",
        "outputId": "e6e32df1-9148-4628-ca28-9b7c02436623"
      },
      "outputs": [],
      "source": [
        "pred_labels, val_losses, dev_acc2, b_labels, _ = classifier_weights.run_on_dataset(dev_data, optimizer, batch_size=32, validation_use=False)\n",
        "pred_labels, val_losses, test_acc2, b_labels, _ = classifier_weights.run_on_dataset(test_data, optimizer, batch_size=32, validation_use=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fGjo_stpUAT7",
        "outputId": "135a38a9-5ca7-4ec9-a8b9-502f739b1e7a"
      },
      "outputs": [],
      "source": [
        "print('dev acc: ', dev_acc2, 'test acc:' , test_acc2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KzKpherTqsj"
      },
      "source": [
        "**Classifier mlp and weights**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I7tpF6H5cArk",
        "outputId": "ae365b05-0e8f-420e-90ff-bb064f79109a"
      },
      "outputs": [],
      "source": [
        "pred_labels, val_losses, dev_acc3, b_labels, _ = classifier_mlp_weights.run_on_dataset(dev_data, optimizer, batch_size=32, validation_use=False)\n",
        "pred_labels, val_losses, test_acc3, b_labels, _ = classifier_mlp_weights.run_on_dataset(test_data, optimizer, batch_size=32, validation_use=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zwrX4nJoUCKS",
        "outputId": "2b07eccf-5fc6-4bdf-bf53-9d253a48c823"
      },
      "outputs": [],
      "source": [
        "print('dev acc: ', dev_acc3, 'test acc:', test_acc3)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "062aa7e54cce40c99ca2e1043f134b90": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "0a05f98a9e1a48729e833652dc413be5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1ffee7d44a3347adbc88c47208679bd6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "200efeb1a36f47b888af30721f1e8a97": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0a05f98a9e1a48729e833652dc413be5",
            "placeholder": "​",
            "style": "IPY_MODEL_496703edea874c6fb8a22fd32e456081",
            "value": " 896k/896k [00:00&lt;00:00, 1.58MB/s]"
          }
        },
        "21b7360121934bc392a719c6eedef583": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "23f0f80a2faa4228a1897b5b92562fff": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "Downloading: 100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ced61c53666f45f48fe301f2af4e5e35",
            "max": 895731,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_062aa7e54cce40c99ca2e1043f134b90",
            "value": 895731
          }
        },
        "2a63baf0cdab419d985d39d07e0b376c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "39f9c710ebd84b6fa81c4b9205c18a15": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "496703edea874c6fb8a22fd32e456081": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "67034dd7a099431d92976a1c2d237e31": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "Downloading: 100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_adb58c36931a412e802c295694410b9c",
            "max": 1561415,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_70315c6a722e4176a2af4b631c755d36",
            "value": 1561415
          }
        },
        "6bebeb3da1ff494196ff8b3c5beb3cff": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "70315c6a722e4176a2af4b631c755d36": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "719b612ae5ab417d81876ae8744cf85f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_23f0f80a2faa4228a1897b5b92562fff",
              "IPY_MODEL_200efeb1a36f47b888af30721f1e8a97"
            ],
            "layout": "IPY_MODEL_39f9c710ebd84b6fa81c4b9205c18a15"
          }
        },
        "8fd286d4947644c5b7f1cc9cb8f5b179": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a15458eba4fe4a3b93dcc3f9494990e3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a5611e1877c246e88f8c9ba7f73fcd14": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b43699041dea4a7bb0f97ccf078e5c8b",
              "IPY_MODEL_bc909bc1b05b4067a947a03dd2708a17"
            ],
            "layout": "IPY_MODEL_2a63baf0cdab419d985d39d07e0b376c"
          }
        },
        "adb58c36931a412e802c295694410b9c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b43699041dea4a7bb0f97ccf078e5c8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "Downloading: 100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a15458eba4fe4a3b93dcc3f9494990e3",
            "max": 1496,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6bebeb3da1ff494196ff8b3c5beb3cff",
            "value": 1496
          }
        },
        "bc909bc1b05b4067a947a03dd2708a17": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_21b7360121934bc392a719c6eedef583",
            "placeholder": "​",
            "style": "IPY_MODEL_fd9fe2c7ea1c4d3b98de684243201383",
            "value": " 1.50k/1.50k [00:02&lt;00:00, 669B/s]"
          }
        },
        "cb2114aab6ab480cb6023fa0e5578c28": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_67034dd7a099431d92976a1c2d237e31",
              "IPY_MODEL_ff41d7c375274c64931a86c6d8c7a0bb"
            ],
            "layout": "IPY_MODEL_ea2cbc25c2504383bbe9ecd164448d84"
          }
        },
        "ced61c53666f45f48fe301f2af4e5e35": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ea2cbc25c2504383bbe9ecd164448d84": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd9fe2c7ea1c4d3b98de684243201383": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ff41d7c375274c64931a86c6d8c7a0bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8fd286d4947644c5b7f1cc9cb8f5b179",
            "placeholder": "​",
            "style": "IPY_MODEL_1ffee7d44a3347adbc88c47208679bd6",
            "value": " 1.56M/1.56M [00:01&lt;00:00, 968kB/s]"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
